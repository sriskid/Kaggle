{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7103119,"sourceType":"datasetVersion","datasetId":4090751}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Dependencies","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport math\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision.models import vgg16","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:41.588098Z","iopub.execute_input":"2024-01-16T05:48:41.588489Z","iopub.status.idle":"2024-01-16T05:48:41.594888Z","shell.execute_reply.started":"2024-01-16T05:48:41.588456Z","shell.execute_reply":"2024-01-16T05:48:41.593805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Device Agnostic Code\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:41.601268Z","iopub.execute_input":"2024-01-16T05:48:41.601552Z","iopub.status.idle":"2024-01-16T05:48:41.607763Z","shell.execute_reply.started":"2024-01-16T05:48:41.601519Z","shell.execute_reply":"2024-01-16T05:48:41.606823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Initialization","metadata":{}},{"cell_type":"code","source":"# View first few rows of train csv\ntrain_df = pd.read_csv(\"/kaggle/input/ripik-hackfest/train/train/train.csv\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:41.609322Z","iopub.execute_input":"2024-01-16T05:48:41.609601Z","iopub.status.idle":"2024-01-16T05:48:41.633283Z","shell.execute_reply.started":"2024-01-16T05:48:41.609577Z","shell.execute_reply":"2024-01-16T05:48:41.632360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dictionary for sparse label to corresponding item for plotting\nlabels_dict = {\n    0: \"crack\",\n    1: \"scratch\",\n    2: \"tire flat\",\n    3: \"dent\",\n    4: \"glass shatter\",\n    5: \"lamp broken\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:41.635080Z","iopub.execute_input":"2024-01-16T05:48:41.635732Z","iopub.status.idle":"2024-01-16T05:48:41.640070Z","shell.execute_reply.started":"2024-01-16T05:48:41.635698Z","shell.execute_reply":"2024-01-16T05:48:41.639164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View data type of train_df\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:41.641357Z","iopub.execute_input":"2024-01-16T05:48:41.641981Z","iopub.status.idle":"2024-01-16T05:48:41.655616Z","shell.execute_reply.started":"2024-01-16T05:48:41.641948Z","shell.execute_reply":"2024-01-16T05:48:41.654441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust the labels to start at 0\ntrain_df[\"label\"] = train_df[\"label\"] - 1","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:41.658171Z","iopub.execute_input":"2024-01-16T05:48:41.658450Z","iopub.status.idle":"2024-01-16T05:48:41.665702Z","shell.execute_reply.started":"2024-01-16T05:48:41.658421Z","shell.execute_reply":"2024-01-16T05:48:41.664766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View class label distribution\ntrain_df[\"label\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:41.666729Z","iopub.execute_input":"2024-01-16T05:48:41.667003Z","iopub.status.idle":"2024-01-16T05:48:41.678638Z","shell.execute_reply.started":"2024-01-16T05:48:41.666981Z","shell.execute_reply":"2024-01-16T05:48:41.677556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The big imbalance in class would indicate a Random Weighted Sampler, data augmentation, or class weights may be needed","metadata":{}},{"cell_type":"markdown","source":"## Sample Data Visualization","metadata":{}},{"cell_type":"code","source":"# Function to View a Few Images\ndef view_sample_images(input_image_dir, dataframe, n_samples=5):\n    sample_df = dataframe.sample(n_samples)\n    num_rows = math.ceil(n_samples / 5)\n    num_cols = 5\n    fig = plt.figure(figsize = (15,10))\n    for i in range(n_samples):\n        ax = plt.subplot(num_rows, num_cols, i+1)\n        file_name = os.path.join(input_image_dir, sample_df.iloc[i][\"filename\"])\n        class_label = sample_df.iloc[i][\"label\"]\n        class_category = labels_dict[class_label]\n        img = cv2.imread(file_name)\n        ax.imshow(img)\n        ax.set_title(f\"Class: {class_label} ({class_category})\")\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:41.679830Z","iopub.execute_input":"2024-01-16T05:48:41.680204Z","iopub.status.idle":"2024-01-16T05:48:41.687816Z","shell.execute_reply.started":"2024-01-16T05:48:41.680137Z","shell.execute_reply":"2024-01-16T05:48:41.686858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View five images\nN_SAMPLES = 5\nview_sample_images(\"/kaggle/input/ripik-hackfest/train/train/images\", train_df, N_SAMPLES)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:41.688944Z","iopub.execute_input":"2024-01-16T05:48:41.689225Z","iopub.status.idle":"2024-01-16T05:48:43.166616Z","shell.execute_reply.started":"2024-01-16T05:48:41.689198Z","shell.execute_reply":"2024-01-16T05:48:43.165667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"# Create Custom Dataset\nclass VehicleImageLoader(Dataset):\n    def __init__(self, dataframe, root_dir, transforms = None):\n        self.annotations = dataframe\n        self.transforms = transforms\n        self.root_dir = root_dir\n    \n    def __len__(self):\n        return len(self.annotations)\n    \n    def  __getitem__(self, idx):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[idx][\"filename\"])\n        img = cv2.imread(img_path)\n        label = self.annotations.iloc[idx][\"label\"]\n        \n        if self.transforms:\n            img = self.transforms(img)\n            \n        return (img, label)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:43.168209Z","iopub.execute_input":"2024-01-16T05:48:43.168573Z","iopub.status.idle":"2024-01-16T05:48:43.177044Z","shell.execute_reply.started":"2024-01-16T05:48:43.168541Z","shell.execute_reply":"2024-01-16T05:48:43.176111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create transforms pipeline\ntransform_pipeline = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224,224)),\n    transforms.ToTensor()\n])","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:43.180956Z","iopub.execute_input":"2024-01-16T05:48:43.181396Z","iopub.status.idle":"2024-01-16T05:48:43.188726Z","shell.execute_reply.started":"2024-01-16T05:48:43.181360Z","shell.execute_reply":"2024-01-16T05:48:43.187750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dataset\ndataset = VehicleImageLoader(train_df, \"/kaggle/input/ripik-hackfest/train/train/images\", transforms = transform_pipeline)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:43.189887Z","iopub.execute_input":"2024-01-16T05:48:43.190195Z","iopub.status.idle":"2024-01-16T05:48:43.197268Z","shell.execute_reply.started":"2024-01-16T05:48:43.190152Z","shell.execute_reply":"2024-01-16T05:48:43.196450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Partition the train dataset into train and validation\nTRAIN_SPLIT = 0.80\n\nnum_train = int(TRAIN_SPLIT * len(dataset))\nnum_validation = len(dataset) - num_train\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [num_train, num_validation])","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:43.198285Z","iopub.execute_input":"2024-01-16T05:48:43.198569Z","iopub.status.idle":"2024-01-16T05:48:43.208990Z","shell.execute_reply.started":"2024-01-16T05:48:43.198541Z","shell.execute_reply":"2024-01-16T05:48:43.208180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the dataloaders\nBATCH_SIZE = 32\n\ntrain_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\nval_dataloader = DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:43.210056Z","iopub.execute_input":"2024-01-16T05:48:43.210351Z","iopub.status.idle":"2024-01-16T05:48:43.217390Z","shell.execute_reply.started":"2024-01-16T05:48:43.210327Z","shell.execute_reply":"2024-01-16T05:48:43.216660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View first batch\ndata_iter = iter(train_dataloader)\nbatch = next(data_iter)\nbatch[0].shape, batch[1].shape","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:43.218354Z","iopub.execute_input":"2024-01-16T05:48:43.218645Z","iopub.status.idle":"2024-01-16T05:48:43.820433Z","shell.execute_reply.started":"2024-01-16T05:48:43.218621Z","shell.execute_reply":"2024-01-16T05:48:43.819429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Modeling","metadata":{}},{"cell_type":"code","source":"# Initialize Pretrained Model\nmodel = vgg16(weights=\"VGG16_Weights.DEFAULT\")","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:43.830647Z","iopub.execute_input":"2024-01-16T05:48:43.830940Z","iopub.status.idle":"2024-01-16T05:48:45.293810Z","shell.execute_reply.started":"2024-01-16T05:48:43.830904Z","shell.execute_reply":"2024-01-16T05:48:45.292923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:45.294950Z","iopub.execute_input":"2024-01-16T05:48:45.295263Z","iopub.status.idle":"2024-01-16T05:48:45.302283Z","shell.execute_reply.started":"2024-01-16T05:48:45.295236Z","shell.execute_reply":"2024-01-16T05:48:45.301352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-tuning model","metadata":{}},{"cell_type":"code","source":"# Change classifier final output\nnum_classes = train_df[\"label\"].nunique()\nmodel.classifier[6] = nn.Linear(in_features = 4096, out_features = num_classes, bias = True)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:45.303402Z","iopub.execute_input":"2024-01-16T05:48:45.303683Z","iopub.status.idle":"2024-01-16T05:48:45.316400Z","shell.execute_reply.started":"2024-01-16T05:48:45.303658Z","shell.execute_reply":"2024-01-16T05:48:45.315552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sent model to device\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:45.317659Z","iopub.execute_input":"2024-01-16T05:48:45.318289Z","iopub.status.idle":"2024-01-16T05:48:45.475733Z","shell.execute_reply.started":"2024-01-16T05:48:45.318233Z","shell.execute_reply":"2024-01-16T05:48:45.474796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimizer and Loss Function","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"min\", factor = 0.2, patience = 2, verbose = True)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:45.476729Z","iopub.execute_input":"2024-01-16T05:48:45.476983Z","iopub.status.idle":"2024-01-16T05:48:45.482868Z","shell.execute_reply.started":"2024-01-16T05:48:45.476960Z","shell.execute_reply":"2024-01-16T05:48:45.481924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_accuracy(y_pred_logits, y_true):\n    predictions = torch.argmax(y_pred_logits, dim = 1)\n    accuracy = accuracy_score(y_true.cpu().numpy(), predictions.cpu().numpy())\n    \n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:45.484036Z","iopub.execute_input":"2024-01-16T05:48:45.484375Z","iopub.status.idle":"2024-01-16T05:48:45.494210Z","shell.execute_reply.started":"2024-01-16T05:48:45.484343Z","shell.execute_reply":"2024-01-16T05:48:45.493315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Basic training function\n# for _ in range(4):\n#     X, y = batch\n#     y_pred = model(X)\n#     loss = criterion(y_pred, y)\n#     print(loss)\n#     accuracy = check_accuracy(y_pred, y)\n#     print(\"Accuracy:\",accuracy)\n#     optimizer.zero_grad()\n#     loss.backward()\n#     optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:45.495393Z","iopub.execute_input":"2024-01-16T05:48:45.496017Z","iopub.status.idle":"2024-01-16T05:48:45.504549Z","shell.execute_reply.started":"2024-01-16T05:48:45.495983Z","shell.execute_reply":"2024-01-16T05:48:45.503665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating Training Function","metadata":{}},{"cell_type":"code","source":"def train_step(model, train_loader, val_loader, optimizer, criterion, num_epochs, total_patience = 5, device = device):\n    train_losses = []\n    val_losses = []\n    train_accuracy = []\n    val_accuracy = []\n    \n    best_val_loss = float(\"inf\")\n    patience = 0\n    \n    model.to(device)\n    for epoch in range(num_epochs):\n        model.train()\n        total_train_loss = 0\n        total_train_acc = 0\n        loop = tqdm(enumerate(train_loader), total = len(train_loader), leave = False)\n        for batch, (X, y) in loop:\n            # Send data to GPU\n            X, y = X.to(device), y.to(device)\n            \n            # Get predictions\n            y_pred = model(X)\n            \n            # Calulcate loss\n            train_loss = criterion(y_pred, y)\n            total_train_loss += train_loss.item()\n            \n            # Get accuracy\n            batch_accuracy = check_accuracy(y_pred, y)\n            total_train_acc += batch_accuracy\n            \n            # Zero optimizer\n            optimizer.zero_grad()\n            \n            # Backpropagation\n            train_loss.backward()\n            \n            # Gradient Descent\n            optimizer.step()\n            \n            # Update Progress Bar\n            loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n            loop.set_postfix(loss = train_loss.item(), acc = batch_accuracy)\n                 \n        # Calculate total loss value (average of all batches)   \n        average_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(average_train_loss)\n        \n        # calculate total accuracy\n        average_train_accuracy = total_train_acc / len(train_loader)\n        train_accuracy.append(average_train_accuracy)\n        \n        # Check Validation\n        model.eval()\n        total_val_loss = 0\n        total_val_acc = 0\n        with torch.inference_mode():\n            val_loop = tqdm(enumerate(val_loader), total = len(val_dataloader), leave = False)\n            for batch, (X,y) in val_loop:\n                X, y = X.to(device), y.to(device)\n                \n                y_pred = model(X)\n                \n                val_loss = criterion(y_pred, y)\n                total_val_loss += val_loss.item()\n                \n                batch_accuracy = check_accuracy(y_pred, y)\n                total_val_acc += batch_accuracy\n                \n                val_loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n                val_loop.set_postfix(loss = val_loss.item(), acc = batch_accuracy)\n                  \n        average_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(average_val_loss)\n        \n        average_val_accuracy = total_val_acc / len(val_loader)\n        val_accuracy.append(average_val_accuracy)\n        \n        \n        # Early Stopping and Model Checkpoint\n        if average_val_loss < best_val_loss:\n            print(f\"Validation Loss Improved From: {best_val_loss} to {average_val_loss}\")\n            best_val_loss = average_val_loss\n            \n            # Save entire model\n            torch.save(model.state_dict(), \"vehicle_classification_weights.pth\")\n            print(\"Saved Weights to vehicle_classification_weights.pth\")\n            \n            patience = 0\n        else:\n            print(\"Validation Loss Did not Improve\")\n            patience += 1\n            \n            if patience == total_patience:\n                print(\"End Training: Early Stopping\")\n                print(f\"Epoch: {epoch+1} | Train Loss: {average_train_loss:.4f} | Validation Loss: {average_val_loss:.4f} | Train Accuracy: {average_train_accuracy:.4f} | Validation Accuracy: {average_val_accuracy:.4f}\")\n                break\n        scheduler.step(average_val_loss)       \n        print(f\"Epoch: {epoch+1} | Train Loss: {average_train_loss:.4f} | Validation Loss: {average_val_loss:.4f} | Train Accuracy: {average_train_accuracy:.4f} | Validation Accuracy: {average_val_accuracy:.4f}\")\n    return {\"Train Accuracy\" : train_accuracy, \"Validation Accuracy\" : val_accuracy, \"Train Loss\" : train_losses, \"Validation Loss\" : val_losses}","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:45.505694Z","iopub.execute_input":"2024-01-16T05:48:45.505964Z","iopub.status.idle":"2024-01-16T05:48:45.523697Z","shell.execute_reply.started":"2024-01-16T05:48:45.505940Z","shell.execute_reply":"2024-01-16T05:48:45.522870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deterministic Behavior\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Training\nNUM_EPOCHS = 20\nhistory = train_step(model, train_dataloader, val_dataloader, optimizer, criterion, NUM_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T05:48:45.528342Z","iopub.execute_input":"2024-01-16T05:48:45.528725Z","iopub.status.idle":"2024-01-16T06:30:44.265473Z","shell.execute_reply.started":"2024-01-16T05:48:45.528699Z","shell.execute_reply":"2024-01-16T06:30:44.264534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save Model and Other Parameters","metadata":{}},{"cell_type":"code","source":"# Save loss and accuracy\ntorch.save(history, \"vehicle_classification_history.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:30:44.266800Z","iopub.execute_input":"2024-01-16T06:30:44.267103Z","iopub.status.idle":"2024-01-16T06:30:44.272636Z","shell.execute_reply.started":"2024-01-16T06:30:44.267074Z","shell.execute_reply":"2024-01-16T06:30:44.271708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:30:44.273840Z","iopub.execute_input":"2024-01-16T06:30:44.274108Z","iopub.status.idle":"2024-01-16T06:30:44.298524Z","shell.execute_reply.started":"2024-01-16T06:30:44.274083Z","shell.execute_reply":"2024-01-16T06:30:44.297619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Performance","metadata":{}},{"cell_type":"code","source":"# Load the model\nmodel = vgg16(weights=\"VGG16_Weights.DEFAULT\")\nmodel.classifier[6] = nn.Linear(in_features = 4096, out_features = num_classes, bias = True)\nmodel.load_state_dict(torch.load(\"vehicle_classification_weights.pth\"))\n\n# Load the history\nhistory = torch.load(\"vehicle_classification_history.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:30:44.299699Z","iopub.execute_input":"2024-01-16T06:30:44.299988Z","iopub.status.idle":"2024-01-16T06:30:46.282592Z","shell.execute_reply.started":"2024-01-16T06:30:44.299951Z","shell.execute_reply":"2024-01-16T06:30:46.281565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Loss and Accuracy","metadata":{}},{"cell_type":"code","source":"# Plot the loss and accuracies\nfig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10,6))\n\n# Loss Values\naxes[0].plot(history[\"Train Loss\"], label = \"Train Loss\")\naxes[0].plot(history[\"Validation Loss\"], label = \"Validation Loss\")\naxes[0].set_title(\"Model Loss\")\naxes[0].set_xlabel(\"Epochs\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].legend()\n\naxes[1].plot(history[\"Train Accuracy\"], label = \"Train Accuracy\")\naxes[1].plot(history[\"Validation Accuracy\"], label = \"Validation Accuracy\")\naxes[1].set_title(\"Model Accuracy\")\naxes[1].set_xlabel(\"Epochs\")\naxes[1].set_ylabel(\"Accuracy\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:30:46.283931Z","iopub.execute_input":"2024-01-16T06:30:46.284240Z","iopub.status.idle":"2024-01-16T06:30:46.939949Z","shell.execute_reply.started":"2024-01-16T06:30:46.284214Z","shell.execute_reply":"2024-01-16T06:30:46.938934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# Get predictions\nmodel.to(device)\nmodel.eval()\ntrue_labels = []\npred_labels = []\n\nwith torch.inference_mode():\n    for batch, (X, y) in enumerate(val_dataloader):\n        X, y = X.to(device), y.to(device)\n        logits = model(X)\n        prediction = torch.argmax(logits, dim = 1)\n        \n        # Append to lists\n        true_labels.extend(y.cpu().numpy())\n        pred_labels.extend(prediction.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:30:46.941448Z","iopub.execute_input":"2024-01-16T06:30:46.941763Z","iopub.status.idle":"2024-01-16T06:31:16.059705Z","shell.execute_reply.started":"2024-01-16T06:30:46.941736Z","shell.execute_reply":"2024-01-16T06:31:16.058648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert lists to NumPy array\ntrue_labels = np.array(true_labels)\npred_labels = np.array(pred_labels)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:31:16.060974Z","iopub.execute_input":"2024-01-16T06:31:16.061290Z","iopub.status.idle":"2024-01-16T06:31:16.066228Z","shell.execute_reply.started":"2024-01-16T06:31:16.061264Z","shell.execute_reply":"2024-01-16T06:31:16.065100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(true_labels, pred_labels)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:31:16.067320Z","iopub.execute_input":"2024-01-16T06:31:16.067595Z","iopub.status.idle":"2024-01-16T06:31:16.079774Z","shell.execute_reply.started":"2024-01-16T06:31:16.067571Z","shell.execute_reply":"2024-01-16T06:31:16.078755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the confusion matrix\nconf_matrix = confusion_matrix(true_labels, pred_labels)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:31:16.080800Z","iopub.execute_input":"2024-01-16T06:31:16.081071Z","iopub.status.idle":"2024-01-16T06:31:16.089571Z","shell.execute_reply.started":"2024-01-16T06:31:16.081047Z","shell.execute_reply":"2024-01-16T06:31:16.088668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the confusion matrix as a seaborn heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(num_classes), yticklabels=range(num_classes))\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:31:16.090948Z","iopub.execute_input":"2024-01-16T06:31:16.091338Z","iopub.status.idle":"2024-01-16T06:31:16.392873Z","shell.execute_reply.started":"2024-01-16T06:31:16.091292Z","shell.execute_reply":"2024-01-16T06:31:16.391936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Predictions","metadata":{}},{"cell_type":"code","source":"# Read in test csv\ntest_df = pd.read_csv(\"/kaggle/input/ripik-hackfest/test/test/test.csv\")\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:31:16.394043Z","iopub.execute_input":"2024-01-16T06:31:16.394314Z","iopub.status.idle":"2024-01-16T06:31:16.409418Z","shell.execute_reply.started":"2024-01-16T06:31:16.394289Z","shell.execute_reply":"2024-01-16T06:31:16.408344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create full paths for all images\ntest_images_dir = \"/kaggle/input/ripik-hackfest/test/test/images/\"\ntest_df[\"filename\"] = test_images_dir + test_df[\"filename\"]","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:31:16.410714Z","iopub.execute_input":"2024-01-16T06:31:16.411055Z","iopub.status.idle":"2024-01-16T06:31:16.416602Z","shell.execute_reply.started":"2024-01-16T06:31:16.411032Z","shell.execute_reply":"2024-01-16T06:31:16.415661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make Predictions","metadata":{}},{"cell_type":"code","source":"def make_predictions(dataframe, model, transforms, num_preds = 5):\n    model.to(\"cpu\")\n    sample_df = dataframe.sample(num_preds)\n    num_rows = math.ceil(num_preds / 5)\n    num_cols = 5\n    fig = plt.figure(figsize = (15,10))\n    for i in range(num_preds):\n        image_path = sample_df.iloc[i][\"filename\"]\n        img = cv2.imread(image_path)\n        ax = plt.subplot(num_rows, num_cols, i+1)\n        img = transforms(img)\n        model.eval()\n        with torch.inference_mode():\n            pred = model(img.unsqueeze(0))\n            pred = torch.argmax(pred, dim = 1)\n        ax.imshow(np.transpose(img, [1,2,0]))\n        ax.set_title(f\"Predicted: {labels_dict[pred.item()]}\")\n        ","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:31:16.417787Z","iopub.execute_input":"2024-01-16T06:31:16.418073Z","iopub.status.idle":"2024-01-16T06:31:16.427580Z","shell.execute_reply.started":"2024-01-16T06:31:16.418049Z","shell.execute_reply":"2024-01-16T06:31:16.426705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_predictions(test_df, model, transform_pipeline)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T06:31:16.428735Z","iopub.execute_input":"2024-01-16T06:31:16.429002Z","iopub.status.idle":"2024-01-16T06:31:18.841200Z","shell.execute_reply.started":"2024-01-16T06:31:16.428978Z","shell.execute_reply":"2024-01-16T06:31:18.840344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}